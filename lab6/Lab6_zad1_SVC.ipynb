{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dane\n",
    "\n",
    "Będziemy pracować na danych dotyczących rodzajów fonacji (nastawienia głosu). Dane zawierają 13 MFCC oraz ich pierwszą i drugą pochodną wyznaczone dla sygnałów zawierających samogłoski emitowane przy użyciu dwóch typów fonacji:\n",
    "\n",
    "- z nastawieniem miękkim - więzadła głosowe zbliżają się do siebie, a ich przyśrodkowe krawędzie swobodnie drgają\n",
    "- z nastawieniem twardym - więzadła głosowe są silnie zwarte przed rozpoczęciem fonacji, tworzy się wysokie ciśnienie podgłośniowe.\n",
    "\n",
    "Rodzaje te są oznaczone odpowiednio etykietami 0 i 1.\n",
    "\n",
    "Źródło danych: https://osf.io/cwquj/. Więcej o rodzajach nastawienia głosu [w artykule autorów zbioru danych](https://research.gold.ac.uk/id/eprint/9621/1/phonation-r11.pdf) lub po polsku [w poradniku higieny głosu (str. 11).]( https://medycynapracyportal.pl/wp-content/uploads/wydawnictwa/poradnik_glos.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, RocCurveDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wczytanie danych\n",
    "X = np.load('padded_features_bin.npy') #macierz cech - jednej wiersz = jeden obiekt\n",
    "y = np.load('labels_bin.npy') #etykiety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42) \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maszyna wektorów nośnych\n",
    "\n",
    "Przeprowadzimy analizę przy użyciu klasyfikatora opartego o maszynę wektorów nośnych [(*support vector machine*, SVM)](https://scikit-learn.org/stable/modules/svm.html#support-vector-machines) (`from sklearn.svm import SVC`). \n",
    "\n",
    "### Przypomnienie:\n",
    "\n",
    "> SVM umożliwia klasyfikację również wtedy, gdy dane nie są liniowo separowalne. Wykorzystywana jest do tego funkcja jądra (*kernel*), która służy do mapowania danych na przestrzeń o wymiarze o jeden większym, gdzie już są (lub przynajmniej powinny być) liniowo separowalne.\n",
    "\n",
    "> SVM dąży nie tylko do rozdzielenia danych należących do różnych klas, ale też zrobienia tego z możliwie dużym marginesem zaufania. O tym, jak wysoka jest kara, która jest następstwem złej klasyfikacji obiektu, oraz jak duży jest margines, informuje parametr C (im mniejsze C, tym bardziej uogólniony model i większy margines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponieważ chcemy wyznaczyć krzywą ROC i ROC AUC (zob. notatnik **Regresja_metryki_ciagle.ipynb**), zamiast metody `predict` użyjemy [`predict_proba`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict_proba) (od *probability*), która zwróci prawdopodobieństwo przynależności do klas (dla każdej klasy w osobnej kolumnie). Tworząc model, musimy więc pamiętać, aby ustawić `probability=True`, żeby zwrócił nam prawdopodobieństwa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SVC(C=1, random_state=42, probability=True) #probability=True pozwala potem użyć metody predict_proba\n",
    "SVM.fit(X_train_scaled, y_train)\n",
    "SVM_train_preds = SVM.predict_proba(X_train_scaled)\n",
    "SVM_test_preds = SVM.predict_proba(X_test_scaled)\n",
    "print('test ROC AUC: ', roc_auc_score(y_test, SVM_test_preds[:,1])) \n",
    "#predict_proba zwraca macierz z prawdopodobieństwiem przynależności do każdej analizowanej klasy\n",
    "#nas interesuje klasa 1, więc bierzemy kolumnę 1, w której są odpowiednie prawdopodobieństwa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Próg klasyfikacji\n",
    "\n",
    "Użycie metody `predict_proba` spowodowało, że nasz klasyfikator (podobnie jak modele regresyjne) zwrócił predykcje ciągłe (wartości prawdopodobieństwa), z których nie można wprost wyliczyć metryk sukcesu. Żeby to zrobić, trzeba je zamienić na liczby całkowite, oznaczające przynależność do klas. W tym celu trzeba przyjąć pewien próg - wartości poniżej niego będą oznaczały przyporządkowanie do klasy 0, a pozostałe do klasy 1.\n",
    "\n",
    "Są różne metody wyznaczania takiego progu - my przyjmiemy wartość, która będzie wyznaczona jako największa średnia geometryczna TPR i (1-FPR) dla całego zbioru danych (czyli w zasadzie potraktujemy zbiór testowy jak ewaluacyjny - służący do poprawy modelu, ale nie jego ostatecznej weryfikacji). Narysujemy wykres z zaznaczonym progiem, żeby łatwiej było sobie wyobrazić, o który dokładnie punkt chodzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = np.append(SVM_train_preds[:,1], SVM_test_preds[:,1], axis=0)\n",
    "all_labels = np.append(y_train, y_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(all_labels, all_preds, pos_label=1) # wyznaczamy krzywą ROC\n",
    "gmeans = np.sqrt(tpr * (1-fpr)) # każdego punktu na krzywej ROC wyznaczamy średnią geometryczną\n",
    "ix = np.argmax(gmeans) # indeks wartości maksymalnej\n",
    "\n",
    "axis = plt.gca()\n",
    "RocCurveDisplay.from_predictions(all_labels, all_preds, ax=axis, plot_chance_level=True) # rysujemy krzywą dla obu zbiorów\n",
    "axis.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best g-mean')\n",
    "plt.legend()\n",
    "print('Best Threshold=%.3f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix])) #G-Mean - największa średnia geometryczna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z wyznaczania progu będziemy jeszcze z tego korzystać, więc zapiszmy go do funkcji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold(y_train, y_test, train_preds, test_preds):\n",
    "    y = np.append(y_train, y_test)\n",
    "    preds = np.append(train_preds, test_preds)\n",
    "    fpr, tpr, thresholds = roc_curve(y, preds)\n",
    "    gmeans = np.sqrt(tpr * (1-fpr)) #średnie geometryczne wyznaczone dla każdego punktu na krzywej ROC\n",
    "    ix = np.argmax(gmeans) # indeks wartości maksymalnej\n",
    "    print('Best Threshold=%.3f' % thresholds[ix])\n",
    "    return thresholds[ix], (fpr[ix], tpr[ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyznaczmy teraz próg klasyfikacji, korzystając ze średniej geometrycznej; generalnie krzywą ROC rysujemy tylko dla zbioru testowego, więc punkt progowy (wyznaczany na podstawie całego zbioru danych) niekoniecznie musi na niej leżeć.\n",
    "\n",
    "Wyświetlimy też wartości prawdopodobieństwa zwrócone przez model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Próg jest na potrzebny, aby otrzymane prawdopodobieństwa zamienić na klasy - wystarczająco wysoka wartość prawdopodobieństwa oznacza, że obiekt możemy zaklasyfikować jako „pozytywny”. Potem możemy wyznaczyć macierz pomyłek i metryki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boolean_SVM_test_preds = \n",
    "\n",
    "print('Threshold', threshold)\n",
    "print(confusion_matrix(y_test, boolean_SVM_test_preds))\n",
    "print('Accuracy', accuracy_score(y_test, boolean_SVM_test_preds))\n",
    "print('Sensitivity/Recall', recall_score(y_test, boolean_SVM_test_preds))\n",
    "print('Precision', precision_score(y_test, boolean_SVM_test_preds))\n",
    "\n",
    "print('test ROC AUC: ', roc_auc_score(y_test, boolean_SVM_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobacz, jak wyniki będą zmieniać się w zależności od dobranego progu - wybierz kilka losowych wartości z przedziału [0,1] i wyznacz macierze pomyłek lub wybraną metrykę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thr in []:\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównaj wyniki uzyskane przez progowanie wartości prawdopodobieństwa z tymi, które uzyskuje się przez klasyfikację metodą `predict`. Czy są takie same? Z jakiej wartości progu korzysta ta metoda?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_test_preds2 = SVM.predict(X_test)\n",
    "print(confusion_matrix(y_test, SVM_test_preds2))\n",
    "print(accuracy_score(y_test, SVM_test_preds2))\n",
    "print(recall_score(y_test, SVM_test_preds2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optymalizacja\n",
    "\n",
    "Dotychczas wszystkie modele tworzyliśmy w taki sposób, że samodzielnie dobieraliśmy wartości hiperparametrów. Zazwyczaj było to robione w sposób losowy.\n",
    "\n",
    "W praktyce wartości hiperparametrów warto dobierać na drodze optymalizacji. Przeprowadzimy taką optymalizację modelu SVM przy użyciu pakietu [**optuna**](https://optuna.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna\n",
    "from optuna import create_study\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walidacja krzyżowa\n",
    "\n",
    "Skorzystamy w tym celu z walidacji krzyżowej (*cross-validation*). Do tej pory wykonywaliśmy walidację prostą - dane były dzielone na zbiór uczący i testowy, a model ocenialiśmy tylko na podstawie metryk wyliczonych dla zbioru testowego.\n",
    "\n",
    "### Przypomnienie\n",
    "\n",
    "> Walidacja krzyżowa jest alternatywnym podejściem do oceny modelu. Dane wykorzystywane w procesie uczenia są dzielone na *k* podzbiorów (tzw. foldów) i uczenie przebiega w następujący sposób:\n",
    "\n",
    "> 1. k-1 podzbiorów tworzy zbiór uczący, na którym uczony jest model, a ostatni pozbiór służy do walidacji (czyli przeprowadzenia predykcji i wyznaczenia metryk);\n",
    "    \n",
    "> 2. proces powtarzany jest k-krotnie, tak by każdy z podzbiorów był użyty do walidacji;\n",
    "    \n",
    "> 3. wyniki ze wszystkich k predykcji są uśredniane;\n",
    "    \n",
    "> 4. można wyliczyć też odchylenie standardowe wyników uzyskanych na wszystkich foldach - wtedy mamy informację, czy model zawsze daje zbliżone wyniki, czy też zdarzyło się, że na którymś foldzie dał dużo niższe niż na innych (nie jest to pożądane).\n",
    "\n",
    "Korzystamy z funkcji [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold), która przy okazji zapewnia stratyfikację danych podczas dzielenia na podzbiory.\n",
    "\n",
    "### Uwaga:\n",
    "\n",
    "Często można spotkać się z tym, że do walidacji krzyżowej użyte są całe posiadane dane (nie wyodrębnia się zbioru testowego). Takie podejście nie jest polecane, ponieważ nie daje pełnego oglądu możliwości modelu. Najbardziej rekomendowane jest prowadzenie walidacji krzyżowej z wykorzystaniem zbioru uczącego (który w jej trakcie jest dzielony na podzbiory treningowy i walidacyjny), a następnie przeprowadzenie dodatkowej oceny modelu z użyciem zbioru testowego - tak będziemy robić na zajęciach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funkcja kosztu (celu)\n",
    "\n",
    "`scoring`\n",
    "\n",
    "Przede wszystkim musimy zdefiniować, jaka metryka będzie służyła do oceny tego, który model (z jakimi hiperparametrami) jest lepszy - „zmapować” zmienne, którymi manipulujemy, do jednej wartości. W przypadku modeli regresyjnych używanych do klasyfikacji jest zazwyczaj to ROC AUC lub *log loss* (w przypadku regresorów dopasowywanych do danych ciągłych najczęściej jest to MSE - *mean squared error* lub MAE - *mean absolute error*). My wykorzystamy ROC AUC.\n",
    "\n",
    "`objective`\n",
    "\n",
    "Kolejną rzeczą, której potrzebujemy, jest funkcja, która nam to mapowanie wykona. Do niej podawany jest model, liczba prób wykonywana podczas całego procesu optymalizacji (losowań wartości hiperparametrów), optymalizowane hiperparametry, oraz dane uczące - zwraca wybraną przez nas metrykę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'roc_macro': make_scorer(roc_auc_score, average='macro')}\n",
    "\n",
    "def objective(trial, model, get_space, X, y):\n",
    "    model_space = get_space(trial)\n",
    "\n",
    "    mdl = model(**model_space)\n",
    "    scores = cross_validate(mdl, X, y,\n",
    "                            scoring=scoring,\n",
    "                            cv=StratifiedKFold(n_splits=5),\n",
    "                            return_train_score=True)\n",
    "\n",
    "    return np.mean(scores['test_roc_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolejną rzeczą, którą należy określić przed dokonaniem optymalizacji, są hiperparametry, które będą optymalizowane oraz zakresy wartości, które będą mogły przyjąć. Na razie będziemy wykorzystywać trzy rodzaje hiperparametrów:\n",
    "\n",
    "1. przyjmujące tylko wartości całkowite, np. liczba iteracji, liczba sąsiadów w metodzie kNN - do zdefiniowania możliwych wartości należy użyć metody `suggest_int()`;\n",
    "    \n",
    "2. przyjmujące wartości kategorialne, np. rodzaj jądra (kernel) - do zdefiniowania możliwych wartości należy użyć metody `suggest_categorical()`;\n",
    "    \n",
    "3. przyjmujące wartości rzeczywiste, często z przedziału [0,1] - do zdefiniowania możliwych wartości należy użyć metody `suggest_float()`.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_space(trial): \n",
    "    space = {\"C\": trial.suggest_float(\"C\", 0, 1.5),  # parametr regularyzacji\n",
    "           'max_iter': trial.suggest_int('max_iter', 100, 100), # maksymalna liczba iteracji do zbiegnięcia modelu\n",
    "           \"kernel\": trial.suggest_categorical(\"kernel\", [\"rbf\", \"linear\", \"poly\"])} # funkcja jądra\n",
    "    return space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodatkowo definiujemy model, którego chcemy użyć (w tym przypadku regresja logistyczna) oraz liczbę prób (triali), które będą wykonane w celu ustalenia najlepszych hiperparametrów. Na razie dajemy małą liczbę prób, żeby nie tracić zbyt dużo czasu - niestety optymalizacja jest bardzo czasochłonna. W przypadku bardziej skomplikowanych modeli, o dużej liczbie hiperparametrów, liczba triali musi być zdecydowanie większa niż 5, żeby sprawdzić różne możliwe kombinacje wartości - może wynosić nawet kilka(dziesiąt) tysięcy.\n",
    "\n",
    "Gdy mamy już wszystko zdefiniowane, możemy w końcu przejść do optymalizacji. Określamy jeszcze, czy metryka stosowana przez nas ma być maksymalizowana czy minimalizowana - chcemy mieć jak największy ROC AUC, więc wybieramy `direction='maximize'` i rozpoczynamy optymalizację."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SVC # bez nawiasów! - przekazujemy klasę, nie tworzymy jej instancji\n",
    "trials = 5 # liczba prób\n",
    "\n",
    "study = create_study(direction='maximize')\n",
    "study.optimize(lambda x: objective(x, model, get_space, X_train_scaled, y_train), n_trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gdy model jest już zoptymalizowany, możemy wyświetlić wartości najlepszych hiperparametrów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('params: ', study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie uczymy model o dobranych hiperparametrach i prowadzimy predykcję, żeby zobaczyć, czy faktycznie uzyskujemy dobre wyniki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_opt = model(**study.best_params) # rozpakowujemy słownik z najlepszymi parametrami\n",
    "\n",
    "SVM_opt.fit(X_train_scaled, y_train)\n",
    "preds = SVM_opt.predict(X_test_scaled)\n",
    "\n",
    "pickle.dump(SVM_opt, open('SVM_model', 'wb+'))\n",
    "print('test ROC_AUC: ', roc_auc_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porównaj uzyskaną wartość ROC AUC z tą, którą uzyskaliśmy samodzielnie dobierając wartości hiperparametrów. Możesz spróbowac zwiększyć liczbę triali, jeżeli wyniki nie są satysfakcjonujące."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uproszczonym wariantem doboru hiperparametrów jest tzw. [*grid search*](https://scikit-learn.org/stable/modules/grid_search.html#grid-search), czyli sprawdzanie różnych kombinacji z góry określonych wartości hiperparametrów."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab-25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
