{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b3822d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torch==2.5.1\n",
    "! pip install pytorch-ignite==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad299ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from ignite.engine import create_supervised_trainer, create_supervised_evaluator, Events\n",
    "from ignite.metrics import Loss, Accuracy\n",
    "from ignite.handlers.tqdm_logger import ProgressBar\n",
    "from ignite.handlers import FastaiLRFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12c122",
   "metadata": {},
   "source": [
    "## Import i przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b191d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb86bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'drive/MyDrive/PUM/Lab8-9/' #zmodyfikuj ścieżkę odpowiednio do lokalizacji plików\n",
    "\n",
    "feats = np.load(folder+'melspec_feats.npy')\n",
    "labels = np.load(folder+'labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8863715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = feats.reshape(feats.shape[0], -1)\n",
    "feats = feats.astype(np.float32)\n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(feats, labels, random_state=42, stratify=labels, train_size=0.8)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, random_state=42, stratify=y_val_test, train_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50760f90",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7836ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "valset = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n",
    "testset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=256)\n",
    "val_loader = DataLoader(valset, batch_size=256)\n",
    "test_loader = DataLoader(testset, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc9978e",
   "metadata": {},
   "source": [
    "## Architektura sieci\n",
    "\n",
    "Oprócz poprawnego procesu uczenia, kiedy sieć stopniowo uczy się wychwytywać zależności w danych, możemy mieć do czynienia z [dwiema niepożądanymi sytuacjami](https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/):\n",
    "* przeuczenie sieci (*overfitting*), czyli zbytnie dopasowanie do zbioru treningowego - model osiąga dobre wyniki na zbiorze treningowym, ale ma słabą zdolność do generalizacji.\n",
    "* niedouczenie sieci (*underfitting*), kiedy model nie jest w stanie wychwycić zależności w danych i osiąga słabe wyniki. Może to znaczyć, że jego architektura jest niedostosowana do stopnia skomplikowania problemu, trening trwał zbyt krótko (za mało epok), lub wybrana reprezentacja danych jest zbyt prosta (np. zbyt mała rozdzielczość częstotliwościowa spektrogramu/szerokość filtra).\n",
    "\n",
    "Do przeuczenia często dochodzi, kiedy zbiór uczący jest zbyt mały. Nie zawsze jesteśmy w stanie go powiększyć o nowe dane, ale możemy zastosować inne [techniki pozwalające zmniejszyć przeuczenie](https://www.pinecone.io/learn/regularization-in-neural-networks/):\n",
    "\n",
    "* augmentacja danych, czyli sztuczne zwiększanie zbioru uczącego poprzez przekształcenia - np. obrót lub odbicie lustrzane, jeśli pracujemy na obrazach, czy dodanie szumu lub pogłosu w przypadku danych audio. Techniki augmentacji dodajemy do dataloadera;\n",
    "![Przykład augmentacji obrazów](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fb9e10cf4e0b69c92426fc686470e85008db934ac-1000x450.png&w=1080&q=75)\n",
    "* *early-stopping*, czyli zakończenie treningu, kiedy funkcja kosztu osiąga minimum;\n",
    "* regularyzacja, czyli modyfikacja funkcji kosztu w taki sposób, aby wprowadzić dodatkowe skalowanie współczynników, zwiększające wagę istotnych cech danych;\n",
    "* ***dropout*** - polega on na losowym „wyłączaniu” neuronów podczas kolejnych iteracji (czyli ustawianiu ich wag na 0) oraz skalowaniu pozostałych przez współczynnik $1/(1-p)$, gdzie $p$ to zadany parametr *dropoutu*. Dzięki temu sieć nie może dopasować się idealnie do danych uczących, ponieważ podczas treningu musi dopasować wagi w taki sposób, by uzyskać dobre rezultaty również wtedy, gdy część neuronów będzie nieaktywna.\n",
    "\n",
    "Definiując warstwę *dropout* określamy prawdopodobieństwo wyłączenia neuronów. Domyślnie $p=0,5$; można tę wartość zmienić, ale należy robić to w sposób przemyślany. Jeżeli ustawimy zbyt dużą wartość prawdopodobieństwa, to sieć może nie być w stanie w ogóle nauczyć się klasyfikować danych. Jeżeli ustawimy wartość zbyt małą, to regularyzacja może być nieefektywna i nie zmniejszymy przeuczenia sieci.\n",
    "\n",
    "Dla utrzymania przejrzystości kodu, jak każdą warstwę, najlepiej jest zdefiniować *dropouty* w funkcji `__init__` i potem odnosić się do nich tworząc połączenia sieci w funkcji `forward`, np.:\n",
    "\n",
    "```\n",
    "def __init__(self):\n",
    "    self.fc1 = nn.Linear(2574, 256)\n",
    "    self.fc2 = nn.Linear(256, 120)\n",
    "    self.fc3 = nn.Linear(120, 35)\n",
    "    self.dropout1 = nn.Dropout(p=0.5)\n",
    "    self.dropout2 = nn.Dropout(p=0.2)\n",
    "    \n",
    "def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6331812f",
   "metadata": {},
   "source": [
    "# Zadanie\n",
    "\n",
    "Sprawdź, czy uda Ci się zmienić architekturę sieci tak, aby uzyskać lepsze wyniki.\n",
    "\n",
    "Możesz to zrobić:\n",
    "- zwiększając liczbę warstw (uwaga na liczbę wejść i wyjść każdej warstwy),\n",
    "- dodając dropout,\n",
    "- zmieniając funkcję aktywacji warstw (wszystkich lub niektórych),\n",
    "- zmieniając optimizer.\n",
    "\n",
    "Po zmianach zwróć uwagę, czy ustalona liczba epok wystarcza do nauczenia sieci - jeżeli do ostatniej epoki obserwujesz wzrost dokładności na zbiorze walidacyjnym, to prawdopodobnie sieć nadal nie jest w pełni nauczona i dłuższy trening przyniesie poprawę wyników. Jeżeli od pewnego momentu treningu obserwujesz spadek dokładności, to sieć zaczęła się przeuczać i trzeba zmniejszyć liczbę epok (uwaga: chodzi o trend spadkowy, a nie pojedyncze wahania)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "615e4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module): # dziedziczenie po klasie nn.Module\n",
    "    def __init__(self,\n",
    "                 n_features, # liczba cech = pasm częstotliwości * ramek\n",
    "                 hidden_in, # liczba neuronów wejściowych warstwy ukrytej\n",
    "                 hidden_out, # liczba neuronów wyjściowych warstwy ukrytej\n",
    "                 n_classes # liczba klas w zbiorze\n",
    "                 ):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, hidden_in)\n",
    "        self.fc2 = nn.Linear(hidden_in, hidden_out)\n",
    "        self.fc3 = nn.Linear(hidden_out, n_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eaa846",
   "metadata": {},
   "source": [
    "## Funkcja straty i optymalizator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37f096d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# tworzymy sieć podając argumenty funkcji init\n",
    "model = Net(n_features=feats.shape[1], #liczba cech = pasm częstotliwości * ramek\n",
    "            hidden_in=256,\n",
    "            hidden_out=120, # liczba neuronów w warstwie pośredniej\n",
    "            n_classes=35 # liczba klas w zbiorze\n",
    "        )\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(device)\n",
    "model.to(device)  # Move model before creating optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b05b6883",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_model_state = model.state_dict()\n",
    "init_opt_state = optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74767f",
   "metadata": {},
   "source": [
    "## Trainer i Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26bfa8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = create_supervised_trainer(model,\n",
    "                                    optimizer,\n",
    "                                    criterion,\n",
    "                                    device=device)\n",
    "\n",
    "evaluator = create_supervised_evaluator(model,\n",
    "                                        metrics={\"acc\": Accuracy(), \"loss\": Loss(nn.NLLLoss())},\n",
    "                                        device=device)\n",
    "\n",
    "ProgressBar(persist=True).attach(trainer, output_transform=lambda x: {\"batch loss\": x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8008b55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_finder = FastaiLRFinder()\n",
    "to_save={'model': model, 'optimizer': optimizer}\n",
    "with lr_finder.attach(trainer, to_save, diverge_th=1.05, start_lr=1e-8, end_lr=1e-3) as trainer_with_lr_finder:\n",
    "    trainer_with_lr_finder.run(train_loader)\n",
    "\n",
    "print(\"Suggested LR\", lr_finder.lr_suggestion())\n",
    "lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ddd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "# evaluator = create_supervised_evaluator(model, metrics={\"acc\": Accuracy(), \"loss\": Loss(nn.NLLLoss())}, device=device)\n",
    "ProgressBar(persist=True).attach(trainer, output_transform=lambda x: {\"batch loss\": x})\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    evaluator.run(val_loader)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n",
    "          .format(trainer.state.epoch, metrics['acc'], metrics['loss']))\n",
    "\n",
    "lr_finder.apply_suggested_lr(optimizer)\n",
    "print('Training with suggested lr: ', optimizer.param_groups[0]['lr'])\n",
    "#trainer.run(trainloader, max_epochs=1000)\n",
    "trainer.run(train_loader, max_epochs=150)\n",
    "\n",
    "evaluator.run(test_loader)\n",
    "print(evaluator.state.metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
